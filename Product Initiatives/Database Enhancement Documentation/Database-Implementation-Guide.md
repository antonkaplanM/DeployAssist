# Database Enhancement Implementation Guide

**Project**: Deployment Assistant - Database Implementation  
**Date**: September 26, 2025  
**Status**: Implementation Guide  
**Based on**: Database Enhancement Proposal  

---

## üéØ Overview

This guide provides detailed step-by-step instructions for implementing the database enhancement to the Deployment Assistant application. The implementation follows a 4-phase approach to minimize risk and ensure smooth deployment.

---

## üìã Pre-Implementation Checklist

### **Environment Requirements**
- [ ] Node.js 18+ installed
- [ ] PostgreSQL 14+ access (local or cloud)
- [ ] Redis 6+ access (local or cloud)
- [ ] Development environment isolated from production
- [ ] Backup of current application state
- [ ] Team access to deployment environments

### **Access Requirements**
- [ ] Database server administrative access
- [ ] Application server deployment permissions
- [ ] Salesforce API credentials for testing
- [ ] JIRA API credentials for testing
- [ ] Version control repository access

### **Documentation Preparation**
- [ ] Current API endpoints documented
- [ ] Data flow diagrams created
- [ ] Rollback procedures defined
- [ ] Testing scenarios documented

---

## üöÄ Phase 1: Foundation Setup (Weeks 1-6)

### **Step 1.1: Database Server Setup**

#### **PostgreSQL Installation & Configuration**

```bash
# Option A: Local Installation (Development)
# Windows (using chocolatey)
choco install postgresql

# macOS (using homebrew)
brew install postgresql
brew services start postgresql

# Linux (Ubuntu/Debian)
sudo apt update
sudo apt install postgresql postgresql-contrib
sudo systemctl start postgresql
sudo systemctl enable postgresql
```

```bash
# Option B: Cloud Setup (Recommended for Production)
# AWS RDS PostgreSQL
# - Go to AWS RDS Console
# - Create PostgreSQL instance (db.t3.medium recommended)
# - Configure VPC and security groups
# - Note endpoint, username, password

# Azure Database for PostgreSQL
# - Go to Azure Portal
# - Create Azure Database for PostgreSQL server
# - Configure networking and firewall rules
# - Note connection string
```

#### **Database Creation & User Setup**

```sql
-- Connect as superuser (postgres)
psql -U postgres -h localhost

-- Create database
CREATE DATABASE deployment_assistant;

-- Create application user
CREATE USER app_user WITH PASSWORD 'your_secure_password_here';

-- Grant permissions
GRANT ALL PRIVILEGES ON DATABASE deployment_assistant TO app_user;

-- Connect to application database
\c deployment_assistant

-- Grant schema permissions
GRANT ALL ON SCHEMA public TO app_user;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO app_user;
GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO app_user;
```

### **Step 1.2: Redis Setup**

#### **Redis Installation**

```bash
# Option A: Local Installation
# Windows (using chocolatey)
choco install redis-64

# macOS (using homebrew)
brew install redis
brew services start redis

# Linux (Ubuntu/Debian)
sudo apt update
sudo apt install redis-server
sudo systemctl start redis-server
sudo systemctl enable redis-server
```

```bash
# Option B: Cloud Setup
# AWS ElastiCache
# - Go to ElastiCache Console
# - Create Redis cluster
# - Note cluster endpoint

# Azure Cache for Redis
# - Go to Azure Portal
# - Create Azure Cache for Redis
# - Note hostname and access keys
```

#### **Redis Configuration**

```bash
# Edit redis.conf (location varies by OS)
# Common locations:
# - Linux: /etc/redis/redis.conf
# - macOS: /usr/local/etc/redis.conf
# - Windows: C:\Program Files\Redis\redis.windows-service.conf

# Key configuration changes:
maxmemory 256mb
maxmemory-policy allkeys-lru
save 900 1
save 300 10
save 60 10000
```

### **Step 1.3: Application Dependencies**

#### **Install Database Packages**

```bash
# Navigate to application directory
cd /path/to/hello-world-nodejs

# Install PostgreSQL driver
npm install pg @types/pg

# Install Redis client
npm install redis @types/redis

# Install connection pooling
npm install pg-pool

# Install database migration tool
npm install db-migrate db-migrate-pg

# Install environment variable management
npm install dotenv

# Install database query builder (optional but recommended)
npm install knex
```

#### **Update package.json**

```json
{
  "dependencies": {
    "pg": "^8.11.3",
    "pg-pool": "^3.6.1",
    "redis": "^4.6.8",
    "db-migrate": "^0.11.13",
    "db-migrate-pg": "^1.5.2",
    "dotenv": "^16.3.1",
    "knex": "^2.5.1"
  },
  "devDependencies": {
    "@types/pg": "^8.10.2",
    "@types/redis": "^4.0.11"
  }
}
```

### **Step 1.4: Environment Configuration**

#### **Create Environment Files**

```bash
# Create .env file in project root
touch .env

# Create .env.example for team reference
touch .env.example
```

**`.env` File Content:**
```env
# Database Configuration
DATABASE_URL=postgresql://app_user:your_secure_password_here@localhost:5432/deployment_assistant
DB_HOST=localhost
DB_PORT=5432
DB_NAME=deployment_assistant
DB_USER=app_user
DB_PASSWORD=your_secure_password_here

# Redis Configuration
REDIS_URL=redis://localhost:6379
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=

# Application Configuration
NODE_ENV=development
PORT=3000

# External API Configuration (existing)
SALESFORCE_USERNAME=your_sf_username
SALESFORCE_PASSWORD=your_sf_password
SALESFORCE_SECURITY_TOKEN=your_sf_token
SALESFORCE_LOGIN_URL=https://login.salesforce.com

JIRA_BASE_URL=https://your-domain.atlassian.net
JIRA_EMAIL=your_jira_email
JIRA_API_TOKEN=your_jira_token

# Cache Configuration
CACHE_TTL_SECONDS=900
CACHE_ENABLED=true
BACKGROUND_SYNC_INTERVAL=300000
```

### **Step 1.5: Database Module Creation**

#### **Create Database Connection Module**

```javascript
// database/connection.js
const { Pool } = require('pg');
const redis = require('redis');
require('dotenv').config();

// PostgreSQL connection pool
const pgPool = new Pool({
    host: process.env.DB_HOST,
    port: process.env.DB_PORT,
    database: process.env.DB_NAME,
    user: process.env.DB_USER,
    password: process.env.DB_PASSWORD,
    max: 20, // Maximum number of connections in pool
    idleTimeoutMillis: 30000,
    connectionTimeoutMillis: 2000,
});

// Redis client
const redisClient = redis.createClient({
    host: process.env.REDIS_HOST,
    port: process.env.REDIS_PORT,
    password: process.env.REDIS_PASSWORD,
    retry_strategy: (options) => {
        if (options.error && options.error.code === 'ECONNREFUSED') {
            console.error('Redis server refused connection');
            return new Error('Redis server refused connection');
        }
        if (options.total_retry_time > 1000 * 60 * 60) {
            console.error('Redis retry time exhausted');
            return new Error('Redis retry time exhausted');
        }
        if (options.attempt > 10) {
            console.error('Redis max attempts reached');
            return undefined;
        }
        return Math.min(options.attempt * 100, 3000);
    }
});

// Connection event handlers
pgPool.on('connect', () => {
    console.log('‚úÖ PostgreSQL connected successfully');
});

pgPool.on('error', (err) => {
    console.error('‚ùå PostgreSQL connection error:', err);
});

redisClient.on('connect', () => {
    console.log('‚úÖ Redis connected successfully');
});

redisClient.on('error', (err) => {
    console.error('‚ùå Redis connection error:', err);
});

// Initialize Redis connection
redisClient.connect().catch(console.error);

module.exports = {
    pgPool,
    redisClient,
    // Test connections
    testConnections: async () => {
        try {
            // Test PostgreSQL
            const pgResult = await pgPool.query('SELECT NOW()');
            console.log('‚úÖ PostgreSQL test query successful:', pgResult.rows[0]);
            
            // Test Redis
            await redisClient.ping();
            console.log('‚úÖ Redis ping successful');
            
            return true;
        } catch (error) {
            console.error('‚ùå Database connection test failed:', error);
            return false;
        }
    }
};
```

### **Step 1.6: Database Schema Implementation**

#### **Create Migration System**

```bash
# Initialize db-migrate
npx db-migrate init

# Create first migration
npx db-migrate create initial-schema
```

#### **Initial Schema Migration**

```javascript
// migrations/20250926000001-initial-schema.js
'use strict';

var dbm;
var type;
var seed;

exports.setup = function(options, seedLink) {
  dbm = options.dbmigrate;
  type = dbm.dataType;
  seed = seedLink;
};

exports.up = function(db) {
  return db.runSql(`
    -- User Sessions Table
    CREATE TABLE user_sessions (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        session_id VARCHAR(255) UNIQUE NOT NULL,
        user_identifier VARCHAR(255) NOT NULL,
        user_preferences JSONB DEFAULT '{}',
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        expires_at TIMESTAMP WITH TIME ZONE NOT NULL,
        is_active BOOLEAN DEFAULT true
    );

    -- API Cache Table
    CREATE TABLE api_cache (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        cache_key VARCHAR(512) UNIQUE NOT NULL,
        cache_value JSONB NOT NULL,
        endpoint VARCHAR(255) NOT NULL,
        query_params JSONB DEFAULT '{}',
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        expires_at TIMESTAMP WITH TIME ZONE NOT NULL,
        hit_count INTEGER DEFAULT 0,
        is_valid BOOLEAN DEFAULT true
    );

    -- Audit Logs Table
    CREATE TABLE audit_logs (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        session_id VARCHAR(255),
        user_identifier VARCHAR(255),
        action VARCHAR(100) NOT NULL,
        resource VARCHAR(255),
        resource_id VARCHAR(255),
        details JSONB DEFAULT '{}',
        ip_address INET,
        user_agent TEXT,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
    );

    -- Validation Configurations Table
    CREATE TABLE validation_configurations (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        user_identifier VARCHAR(255) NOT NULL,
        rule_id VARCHAR(100) NOT NULL,
        is_enabled BOOLEAN DEFAULT true,
        configuration JSONB DEFAULT '{}',
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(user_identifier, rule_id)
    );

    -- Validation Results Table
    CREATE TABLE validation_results (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        run_id UUID NOT NULL,
        record_id VARCHAR(255) NOT NULL,
        record_name VARCHAR(255),
        rule_id VARCHAR(100) NOT NULL,
        rule_name VARCHAR(255) NOT NULL,
        status VARCHAR(20) NOT NULL CHECK (status IN ('PASS', 'FAIL', 'SKIP', 'ERROR')),
        error_message TEXT,
        details JSONB DEFAULT '{}',
        execution_time_ms INTEGER,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
    );

    -- Validation Runs Table
    CREATE TABLE validation_runs (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        user_identifier VARCHAR(255),
        trigger_type VARCHAR(50) NOT NULL,
        time_frame VARCHAR(20),
        total_records INTEGER DEFAULT 0,
        passed_records INTEGER DEFAULT 0,
        failed_records INTEGER DEFAULT 0,
        total_rules INTEGER DEFAULT 0,
        execution_time_ms INTEGER,
        started_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        completed_at TIMESTAMP WITH TIME ZONE,
        status VARCHAR(20) DEFAULT 'RUNNING' CHECK (status IN ('RUNNING', 'COMPLETED', 'FAILED', 'CANCELLED'))
    );

    -- Create Indexes
    CREATE INDEX idx_user_sessions_session_id ON user_sessions(session_id);
    CREATE INDEX idx_user_sessions_user_identifier ON user_sessions(user_identifier);
    CREATE INDEX idx_user_sessions_expires_at ON user_sessions(expires_at);

    CREATE INDEX idx_api_cache_cache_key ON api_cache(cache_key);
    CREATE INDEX idx_api_cache_endpoint ON api_cache(endpoint);
    CREATE INDEX idx_api_cache_expires_at ON api_cache(expires_at);

    CREATE INDEX idx_audit_logs_session_id ON audit_logs(session_id);
    CREATE INDEX idx_audit_logs_user_identifier ON audit_logs(user_identifier);
    CREATE INDEX idx_audit_logs_action ON audit_logs(action);
    CREATE INDEX idx_audit_logs_created_at ON audit_logs(created_at);

    CREATE INDEX idx_validation_configurations_user_identifier ON validation_configurations(user_identifier);
    CREATE INDEX idx_validation_configurations_rule_id ON validation_configurations(rule_id);

    CREATE INDEX idx_validation_results_run_id ON validation_results(run_id);
    CREATE INDEX idx_validation_results_record_id ON validation_results(record_id);
    CREATE INDEX idx_validation_results_rule_id ON validation_results(rule_id);
    CREATE INDEX idx_validation_results_status ON validation_results(status);

    CREATE INDEX idx_validation_runs_user_identifier ON validation_runs(user_identifier);
    CREATE INDEX idx_validation_runs_started_at ON validation_runs(started_at);
    CREATE INDEX idx_validation_runs_status ON validation_runs(status);

    -- Create foreign key for validation results
    ALTER TABLE validation_results 
    ADD CONSTRAINT fk_validation_results_run_id 
    FOREIGN KEY (run_id) REFERENCES validation_runs(id) ON DELETE CASCADE;
  `);
};

exports.down = function(db) {
  return db.runSql(`
    DROP TABLE IF EXISTS validation_results CASCADE;
    DROP TABLE IF EXISTS validation_runs CASCADE;
    DROP TABLE IF EXISTS validation_configurations CASCADE;
    DROP TABLE IF EXISTS audit_logs CASCADE;
    DROP TABLE IF EXISTS api_cache CASCADE;
    DROP TABLE IF EXISTS user_sessions CASCADE;
  `);
};

exports._meta = {
  "version": 1
};
```

#### **Run Migration**

```bash
# Set database configuration for migrations
export DATABASE_URL="postgresql://app_user:your_secure_password_here@localhost:5432/deployment_assistant"

# Run migration
npx db-migrate up

# Verify migration
psql -U app_user -d deployment_assistant -c "\dt"
```

### **Step 1.7: Basic Cache Implementation**

#### **Create Cache Service**

```javascript
// services/cacheService.js
const { redisClient } = require('../database/connection');

class CacheService {
    constructor() {
        this.defaultTTL = parseInt(process.env.CACHE_TTL_SECONDS) || 900; // 15 minutes
        this.enabled = process.env.CACHE_ENABLED === 'true';
    }

    // Generate cache key
    generateKey(endpoint, params = {}) {
        const sortedParams = Object.keys(params).sort().reduce((result, key) => {
            result[key] = params[key];
            return result;
        }, {});
        return `cache:${endpoint}:${Buffer.from(JSON.stringify(sortedParams)).toString('base64')}`;
    }

    // Get cached data
    async get(endpoint, params = {}) {
        if (!this.enabled) return null;
        
        try {
            const key = this.generateKey(endpoint, params);
            const cached = await redisClient.get(key);
            
            if (cached) {
                console.log(`üéØ Cache HIT for ${endpoint}`);
                await this.incrementHitCount(key);
                return JSON.parse(cached);
            }
            
            console.log(`üí® Cache MISS for ${endpoint}`);
            return null;
        } catch (error) {
            console.error('‚ùå Cache get error:', error);
            return null;
        }
    }

    // Set cached data
    async set(endpoint, params = {}, data, ttl = null) {
        if (!this.enabled) return false;
        
        try {
            const key = this.generateKey(endpoint, params);
            const expiry = ttl || this.defaultTTL;
            
            await redisClient.setEx(key, expiry, JSON.stringify(data));
            
            // Store cache metadata in PostgreSQL
            await this.storeCacheMetadata(key, endpoint, params, expiry);
            
            console.log(`üíæ Cache SET for ${endpoint} (TTL: ${expiry}s)`);
            return true;
        } catch (error) {
            console.error('‚ùå Cache set error:', error);
            return false;
        }
    }

    // Invalidate cache for endpoint
    async invalidate(endpoint, params = null) {
        try {
            if (params) {
                // Invalidate specific cache entry
                const key = this.generateKey(endpoint, params);
                await redisClient.del(key);
                console.log(`üóëÔ∏è Cache invalidated for ${endpoint} with specific params`);
            } else {
                // Invalidate all cache entries for endpoint
                const pattern = `cache:${endpoint}:*`;
                const keys = await redisClient.keys(pattern);
                if (keys.length > 0) {
                    await redisClient.del(keys);
                    console.log(`üóëÔ∏è Cache invalidated for ${endpoint} (${keys.length} entries)`);
                }
            }
            return true;
        } catch (error) {
            console.error('‚ùå Cache invalidation error:', error);
            return false;
        }
    }

    // Store cache metadata in PostgreSQL
    async storeCacheMetadata(cacheKey, endpoint, queryParams, ttl) {
        const { pgPool } = require('../database/connection');
        
        try {
            const expiresAt = new Date(Date.now() + (ttl * 1000));
            
            await pgPool.query(`
                INSERT INTO api_cache (cache_key, cache_value, endpoint, query_params, expires_at)
                VALUES ($1, $2, $3, $4, $5)
                ON CONFLICT (cache_key) 
                DO UPDATE SET 
                    cache_value = EXCLUDED.cache_value,
                    expires_at = EXCLUDED.expires_at,
                    hit_count = 0,
                    is_valid = true
            `, [cacheKey, '{}', endpoint, JSON.stringify(queryParams), expiresAt]);
            
        } catch (error) {
            console.error('‚ùå Error storing cache metadata:', error);
        }
    }

    // Increment hit count
    async incrementHitCount(cacheKey) {
        const { pgPool } = require('../database/connection');
        
        try {
            await pgPool.query(`
                UPDATE api_cache 
                SET hit_count = hit_count + 1 
                WHERE cache_key = $1
            `, [cacheKey]);
        } catch (error) {
            console.error('‚ùå Error incrementing hit count:', error);
        }
    }

    // Clean expired cache entries
    async cleanExpired() {
        try {
            const { pgPool } = require('../database/connection');
            
            // Get expired cache keys from PostgreSQL
            const result = await pgPool.query(`
                SELECT cache_key 
                FROM api_cache 
                WHERE expires_at < CURRENT_TIMESTAMP
            `);
            
            // Delete from Redis
            if (result.rows.length > 0) {
                const keys = result.rows.map(row => row.cache_key);
                await redisClient.del(keys);
                
                // Update PostgreSQL
                await pgPool.query(`
                    UPDATE api_cache 
                    SET is_valid = false 
                    WHERE expires_at < CURRENT_TIMESTAMP
                `);
                
                console.log(`üßπ Cleaned ${keys.length} expired cache entries`);
            }
            
        } catch (error) {
            console.error('‚ùå Error cleaning expired cache:', error);
        }
    }

    // Get cache statistics
    async getStats() {
        try {
            const { pgPool } = require('../database/connection');
            
            const result = await pgPool.query(`
                SELECT 
                    endpoint,
                    COUNT(*) as total_entries,
                    SUM(hit_count) as total_hits,
                    AVG(hit_count) as avg_hits,
                    COUNT(CASE WHEN expires_at > CURRENT_TIMESTAMP THEN 1 END) as active_entries
                FROM api_cache 
                WHERE is_valid = true
                GROUP BY endpoint
                ORDER BY total_hits DESC
            `);
            
            return result.rows;
        } catch (error) {
            console.error('‚ùå Error getting cache stats:', error);
            return [];
        }
    }
}

module.exports = new CacheService();
```

### **Step 1.8: Application Integration**

#### **Update app.js with Database Support**

```javascript
// Add to top of app.js
require('dotenv').config();
const { testConnections } = require('./database/connection');
const cacheService = require('./services/cacheService');

// Add after existing requires but before route definitions
// Test database connections on startup
testConnections().then(success => {
    if (success) {
        console.log('‚úÖ All database connections established');
        
        // Start cache cleanup interval
        setInterval(() => {
            cacheService.cleanExpired();
        }, 300000); // Every 5 minutes
        
    } else {
        console.error('‚ùå Database connection failed - some features may not work');
    }
}).catch(console.error);
```

### **Step 1.9: Testing & Validation**

#### **Create Database Test Script**

```javascript
// tests/database-test.js
const { testConnections } = require('../database/connection');
const cacheService = require('../services/cacheService');

async function runDatabaseTests() {
    console.log('üß™ Running database tests...\n');
    
    try {
        // Test 1: Connection test
        console.log('Test 1: Database connections');
        const connectionsOk = await testConnections();
        console.log(connectionsOk ? '‚úÖ PASS' : '‚ùå FAIL');
        
        // Test 2: Cache functionality
        console.log('\nTest 2: Cache functionality');
        await cacheService.set('/test', {}, { message: 'test data' }, 60);
        const cached = await cacheService.get('/test', {});
        const cacheOk = cached && cached.message === 'test data';
        console.log(cacheOk ? '‚úÖ PASS' : '‚ùå FAIL');
        
        // Test 3: Cache invalidation
        console.log('\nTest 3: Cache invalidation');
        await cacheService.invalidate('/test');
        const invalidated = await cacheService.get('/test', {});
        const invalidationOk = invalidated === null;
        console.log(invalidationOk ? '‚úÖ PASS' : '‚ùå FAIL');
        
        // Test 4: Database schema verification
        console.log('\nTest 4: Database schema');
        const { pgPool } = require('../database/connection');
        const schemaResult = await pgPool.query(`
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = 'public' 
            ORDER BY table_name
        `);
        
        const expectedTables = [
            'api_cache', 'audit_logs', 'user_sessions', 
            'validation_configurations', 'validation_results', 'validation_runs'
        ];
        
        const actualTables = schemaResult.rows.map(row => row.table_name);
        const schemaOk = expectedTables.every(table => actualTables.includes(table));
        console.log(schemaOk ? '‚úÖ PASS' : '‚ùå FAIL');
        
        if (!schemaOk) {
            console.log('Expected tables:', expectedTables);
            console.log('Actual tables:', actualTables);
        }
        
        console.log('\nüéâ Database tests completed!');
        
    } catch (error) {
        console.error('‚ùå Database test error:', error);
    }
}

// Run tests if called directly
if (require.main === module) {
    runDatabaseTests().then(() => process.exit(0));
}

module.exports = { runDatabaseTests };
```

#### **Run Tests**

```bash
# Test database setup
node tests/database-test.js

# Start application with database support
npm start

# Verify application loads without errors
curl http://localhost:3000/api/test
```

---

## üìä Phase 1 Deliverables Checklist

- [ ] PostgreSQL database installed and configured
- [ ] Redis cache server installed and configured
- [ ] Database schema created with all tables
- [ ] Database connection pooling implemented
- [ ] Basic cache service implemented
- [ ] Environment configuration completed
- [ ] Migration system established
- [ ] Connection testing completed
- [ ] Application starts without database errors
- [ ] Cache functionality verified

---

## ‚ö†Ô∏è Phase 1 Troubleshooting

### **Common Issues & Solutions**

#### **PostgreSQL Connection Issues**
```bash
# Check if PostgreSQL is running
sudo systemctl status postgresql  # Linux
brew services list | grep postgres  # macOS

# Test manual connection
psql -U app_user -d deployment_assistant -h localhost -p 5432

# Check logs
tail -f /var/log/postgresql/postgresql-14-main.log  # Linux
```

#### **Redis Connection Issues**
```bash
# Check if Redis is running
redis-cli ping

# Check Redis logs
tail -f /var/log/redis/redis-server.log  # Linux

# Test Redis connection
redis-cli -h localhost -p 6379
```

#### **Environment Variables Issues**
```bash
# Verify .env file is loaded
node -e "require('dotenv').config(); console.log(process.env.DATABASE_URL)"

# Check file permissions
ls -la .env
```

---

This completes Phase 1 of the database enhancement implementation. The foundation is now in place for Phase 2, which will focus on implementing user sessions, audit logging, and core database features.

Would you like me to continue with the detailed steps for Phase 2: Core Features Implementation?



